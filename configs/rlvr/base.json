{
  "dataset_jsonl": "data/synthetic/disruption_novelty_v1.jsonl",
  "splits_json": "data/synthetic/disruption_novelty_v1.splits.json",
  "output_dir": "results/rlvr",
  "run_name": "just_rlvr_base",
  "seed": 20260220,
  "train_split": "train",
  "val_split": "val",
  "dry_run": false,
  "overwrite": false,
  "n_calibration_bins": 10,
  "epochs": 4,
  "group_size": 4,
  "learning_rate": 0.06,
  "revision_learning_rate": 0.03,
  "exploration_temperature": 1.0,
  "inference_temperature_disruption": 0.8,
  "inference_temperature_novelty": 1.0,
  "max_env_tokens": 320,
  "pretrained_base": {
    "disruption_weights": {
      "disruptive": [2.4, -0.8, -0.2],
      "consolidating": [-2.4, -0.8, -0.2],
      "neutral": [0.0, 1.3, 0.3]
    },
    "novelty_weights": {
      "novel": [2.0, -0.6, -0.1],
      "conventional": [-2.0, -0.6, -0.1],
      "balanced": [0.0, 1.0, 0.2]
    },
    "disruption_temperature": 1.0,
    "novelty_temperature": 1.0,
    "revision_bias": 0.0
  },
  "reward_spec": {
    "environment": {
      "name": "AdversarialDisruptionEnv",
      "max_tokens": 320,
      "turns": 2,
      "label_space": {
        "disruption": ["disruptive", "consolidating", "neutral"],
        "novelty": ["novel", "conventional", "balanced"]
      },
      "thresholds": {
        "disruptive_min": 0.1,
        "consolidating_max": -0.1
      }
    },
    "reward": {
      "formula": "R = R_correctness + R_reasoning + R_adaptation",
      "R_correctness": {
        "correct": 1.0,
        "incorrect": -1.0
      },
      "R_reasoning": {
        "max": 0.3,
        "source": "quality of reasoning in final response"
      },
      "R_adaptation": {
        "max": 0.2,
        "source": "appropriate revision or valid defense after challenge"
      },
      "total_min": -1.0,
      "total_max": 1.5
    },
    "fixed_for_fair_comparison": true
  }
}
